# Evaluation

The file `run_evalution.py` takes care of the evaluation of the outputs generated by
the LLMs and can be run in the following way: `python run_evaluation.py -d [path_data_to_eval]`.

The model outputs that should be evaluated can be specified in a json file of the following form:

```
{
  "data_to_eval": [
    {
      "generated_plans_path": [PATH TO OUTPUT DIR],
      "evaluation_results_file": [PATH TO EVALUATION FILE],
      "gold_plan_dir": [PATH TO DIR WITH GOLD PLANS],
      "is_complete_plan": True | False
    },
    {
      "generated_plans_path": [PATH TO OUTPUT DIR],
      "evaluation_results_file": [PATH TO EVALUATION FILE],
      "gold_plan_dir": [PATH TO DIR WITH GOLD PLANS],
      "is_complete_plan": True | False
    },
    ...
   }
```
where 
* "generated_plans_path": the path to a directory containing the output files of running the LLM planning
* "evaluation_results_file": the path to the file with the evaluation results that gets created
* "gold_plan_dir": path to the directory containing all gold plans for each of the instances in "generated_plans_path" directory
* "is_complete_plan": True if the plans were generated using an non-interactive planning approache, False otherwise

